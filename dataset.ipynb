{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4F0ucGiy6Vs4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1561,
     "status": "ok",
     "timestamp": 1747031366056,
     "user": {
      "displayName": "이채훈",
      "userId": "14976066403014206527"
     },
     "user_tz": -540
    },
    "id": "6Bn1jW8rGIhQ",
    "outputId": "a91e1383-dcba-4fb8-e7b3-9e92c42d17c3"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# import os\n",
    "# os.chdir('/content/drive/MyDrive/Colab Notebooks/swishnet')\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"_16K_32000cut\"\n",
    "suffix_random = \"_16K_random_cut\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1g_JU8dEGThG"
   },
   "outputs": [],
   "source": [
    "class train_data(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_car_data_dir = f'./train/car/train_car_data{suffix}.pt'\n",
    "        self.train_music_data_dir = f'./train/music/train_music_data{suffix}.pt'\n",
    "        self.train_noise_data_dir = f'./train/noise/train_noise_data{suffix}.pt'\n",
    "        self.train_speech_data_dir = f'./train/speech/train_speech_data{suffix}.pt'\n",
    "\n",
    "        self.car_data = torch.load(self.train_car_data_dir)\n",
    "        self.music_data = torch.load(self.train_music_data_dir)\n",
    "        self.noise_data = torch.load(self.train_noise_data_dir)\n",
    "        self.speech_data = torch.load(self.train_speech_data_dir)\n",
    "        \n",
    "        for i in range(len(self.car_data)):\n",
    "            self.car_data[i] = (self.car_data[i], 0)    \n",
    "        for i in range(len(self.music_data)):\n",
    "            self.music_data[i] = (self.music_data[i], 1)\n",
    "        for i in range(len(self.speech_data)):\n",
    "            self.speech_data[i] = (self.speech_data[i], 2)\n",
    "        for i in range(len(self.noise_data)):\n",
    "            self.noise_data[i] = (self.noise_data[i], 3)\n",
    "\n",
    "        self.data = self.car_data + self.music_data + self.speech_data + self.noise_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,x):\n",
    "        return self.data[x][0], self.data[x][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kN-Li8jCWKN_"
   },
   "outputs": [],
   "source": [
    "class eval_data(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eval_car_data_dir = f'./eval/car/eval_car_data{suffix}.pt'\n",
    "        self.eval_music_data_dir = f'./eval/music/eval_music_data{suffix}.pt'\n",
    "        self.eval_noise_data_dir = f'./eval/noise/eval_noise_data{suffix}.pt'\n",
    "        self.eval_speech_data_dir = f'./eval/speech/eval_speech_data{suffix}.pt'\n",
    "\n",
    "        self.car_data = torch.load(self.eval_car_data_dir)\n",
    "        self.music_data = torch.load(self.eval_music_data_dir)\n",
    "        self.noise_data = torch.load(self.eval_noise_data_dir)\n",
    "        self.speech_data = torch.load(self.eval_speech_data_dir)\n",
    "\n",
    "        for i in range(len(self.car_data)):\n",
    "            self.car_data[i] = (self.car_data[i], 0)    \n",
    "        for i in range(len(self.music_data)):\n",
    "            self.music_data[i] = (self.music_data[i], 1)\n",
    "        for i in range(len(self.speech_data)):\n",
    "            self.speech_data[i] = (self.speech_data[i], 2)\n",
    "        for i in range(len(self.noise_data)):\n",
    "            self.noise_data[i] = (self.noise_data[i], 3)\n",
    "\n",
    "        self.data = self.car_data + self.music_data + self.speech_data + self.noise_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,x):\n",
    "        return self.data[x][0], self.data[x][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_data_random(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_car_data_dir = f'./train/car/train_car_data{suffix_random}.pt'\n",
    "        self.train_music_data_dir = f'./train/music/train_music_data{suffix_random}.pt'\n",
    "        self.train_noise_data_dir = f'./train/noise/train_noise_data{suffix_random}.pt'\n",
    "        self.train_speech_data_dir = f'./train/speech/train_speech_data{suffix_random}.pt'\n",
    "\n",
    "        self.car_data = torch.load(self.train_car_data_dir)\n",
    "        self.music_data = torch.load(self.train_music_data_dir)\n",
    "        self.noise_data = torch.load(self.train_noise_data_dir)\n",
    "        self.speech_data = torch.load(self.train_speech_data_dir)\n",
    "        \n",
    "        for i in range(len(self.car_data)):\n",
    "            self.car_data[i] = (self.car_data[i], 0)    \n",
    "        for i in range(len(self.music_data)):\n",
    "            self.music_data[i] = (self.music_data[i], 1)\n",
    "        for i in range(len(self.speech_data)):\n",
    "            self.speech_data[i] = (self.speech_data[i], 2)\n",
    "        for i in range(len(self.noise_data)):\n",
    "            self.noise_data[i] = (self.noise_data[i], 3)\n",
    "\n",
    "        self.data = self.car_data + self.music_data + self.speech_data + self.noise_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, x):\n",
    "        waveform, label = self.data[x]\n",
    "        target_length = 48000  # 3초 @ 16kHz\n",
    "\n",
    "        # padding 필요할 경우 뒤에 0-padding\n",
    "        if waveform.shape[0] < target_length:\n",
    "            pad_len = target_length - waveform.shape[0]\n",
    "            waveform = F.pad(waveform, (0, pad_len))  # (left, right) padding\n",
    "        elif waveform.shape[0] > target_length:\n",
    "            waveform = waveform[:target_length]  # 혹시나 너무 길 경우 자름\n",
    "\n",
    "        return waveform, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eval_data_random(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eval_car_data_dir = f'./eval/car/eval_car_data{suffix_random}.pt'\n",
    "        self.eval_music_data_dir = f'./eval/music/eval_music_data{suffix_random}.pt'\n",
    "        self.eval_noise_data_dir = f'./eval/noise/eval_noise_data{suffix_random}.pt'\n",
    "        self.eval_speech_data_dir = f'./eval/speech/eval_speech_data{suffix_random}.pt'\n",
    "\n",
    "        self.car_data = torch.load(self.eval_car_data_dir)\n",
    "        self.music_data = torch.load(self.eval_music_data_dir)\n",
    "        self.noise_data = torch.load(self.eval_noise_data_dir)\n",
    "        self.speech_data = torch.load(self.eval_speech_data_dir)\n",
    "\n",
    "        for i in range(len(self.car_data)):\n",
    "            self.car_data[i] = (self.car_data[i], 0)    \n",
    "        for i in range(len(self.music_data)):\n",
    "            self.music_data[i] = (self.music_data[i], 1)\n",
    "        for i in range(len(self.speech_data)):\n",
    "            self.speech_data[i] = (self.speech_data[i], 2)\n",
    "        for i in range(len(self.noise_data)):\n",
    "            self.noise_data[i] = (self.noise_data[i], 3)\n",
    "\n",
    "        self.data = self.car_data + self.music_data + self.speech_data + self.noise_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,x):\n",
    "        waveform, label = self.data[x]\n",
    "        target_length = 48000  # 3초 @ 16kHz\n",
    "\n",
    "        # padding 필요할 경우 뒤에 0-padding\n",
    "        if waveform.shape[0] < target_length:\n",
    "            pad_len = target_length - waveform.shape[0]\n",
    "            waveform = F.pad(waveform, (0, pad_len))  # (left, right) padding\n",
    "        elif waveform.shape[0] > target_length:\n",
    "            waveform = waveform[:target_length]  # 혹시나 너무 길 경우 자름\n",
    "\n",
    "        return waveform, label"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMvbrUS8pHkNguZI7ynmNPo",
   "mount_file_id": "15WwKC6011x1WJjoK5zlGovbNZQfmne1S",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
